\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:introduction}{{1}{2}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simplified diagram of the general model building procedure for pattern classification.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:learning_model}{{1}{2}{A simplified diagram of the general model building procedure for pattern classification.\relax }{figure.caption.2}{}}
\citation{rish2001empirical}
\citation{domingos1997optimality}
\citation{kazmierska2008application}
\citation{wang2007naive}
\citation{sahami1998bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {2}Naive Bayes Classification}{3}{section.2}}
\newlabel{sec:naive_bayes_classification}{{2}{3}{Naive Bayes Classification}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Overview}{3}{subsection.2.1}}
\newlabel{sec:overview}{{2.1}{3}{Overview}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Posterior Probabilities}{3}{subsection.2.2}}
\newlabel{sec:posterior_probabilities_1}{{2.2}{3}{Posterior Probabilities}{subsection.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Linear (A) vs. non-linear problems (B). Random samples for two different classes are shown as colored spheres, and the dotted lines indicate the class boundaries that classifiers try to approximate by computing the decision boundaries. A non-linear problem (B) would be a case where linear classifiers, such as naive Bayes, would not be suitable since the classes are not linearly separable. In such a scenario, non-linear classifiers (e.g.,instance-based nearest neighbor classifiers) should be preferred.\relax }}{4}{figure.caption.3}}
\newlabel{fig:nonlinear_probs}{{2}{4}{Linear (A) vs. non-linear problems (B). Random samples for two different classes are shown as colored spheres, and the dotted lines indicate the class boundaries that classifiers try to approximate by computing the decision boundaries. A non-linear problem (B) would be a case where linear classifiers, such as naive Bayes, would not be suitable since the classes are not linearly separable. In such a scenario, non-linear classifiers (e.g.,instance-based nearest neighbor classifiers) should be preferred.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Class-conditional Probabilities}{5}{subsection.2.3}}
\newlabel{sec:class-conditional_probabilities_1}{{2.3}{5}{Class-conditional Probabilities}{subsection.2.3}{}}
\citation{zhang2004optimality}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Prior Probabilities}{6}{subsection.2.4}}
\newlabel{sec:prior_probabilities_1}{{2.4}{6}{Prior Probabilities}{subsection.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The effect of prior probabilities on the decision regions. The figure shows a 1-dimensional random sample from two different classes (blue and green crosses). The data points for both classes are normally distributed with standard deviation 1, and the bell curves denote the class conditional probabilities. If the class priors are equal, the decision boundary of a naive Bayes classifier is placed at the center between both distributions (gray bar). An increase of the class-conditional probability of the blue class ($\omega _2$) leads to an extension of the decision region R1 by moving the decision boundary (blue-dotted bar) towards the other class and vice versa.\relax }}{8}{figure.caption.4}}
\newlabel{fig:effect_priors}{{3}{8}{The effect of prior probabilities on the decision regions. The figure shows a 1-dimensional random sample from two different classes (blue and green crosses). The data points for both classes are normally distributed with standard deviation 1, and the bell curves denote the class conditional probabilities. If the class priors are equal, the decision boundary of a naive Bayes classifier is placed at the center between both distributions (gray bar). An increase of the class-conditional probability of the blue class ($\omega _2$) leads to an extension of the decision region R1 by moving the decision boundary (blue-dotted bar) towards the other class and vice versa.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Evidence}{8}{subsection.2.5}}
\newlabel{sec:evidence}{{2.5}{8}{Evidence}{subsection.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Multinomial Naive Bayes - A Toy Example}{9}{subsection.2.6}}
\newlabel{sec:multinomial_naive_bayes-a_toy_example}{{2.6}{9}{Multinomial Naive Bayes - A Toy Example}{subsection.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A simple toy dataset of 12 samples 2 different classes $+, -$ . Each sample consists of 2 features: color and geometrical shape.\relax }}{9}{figure.caption.5}}
\newlabel{fig:toy_dataset}{{4}{9}{A simple toy dataset of 12 samples 2 different classes $+, -$ . Each sample consists of 2 features: color and geometrical shape.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A new sample from class $+$ and the features $\textbf  {x} = \text  {[blue, square]}$ that is to be classified using the training data in Figure \ref  {fig:toy_dataset}.\relax }}{10}{figure.caption.6}}
\newlabel{fig:new_sample1}{{5}{10}{A new sample from class $+$ and the features $\textbf {x} = \text {[blue, square]}$ that is to be classified using the training data in Figure \ref {fig:toy_dataset}.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Maximum-Likelihood Estimates}{10}{subsubsection.2.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Classification}{11}{subsubsection.2.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Additive Smoothening}{11}{subsubsection.2.6.3}}
\newlabel{sec:additive_smoothening}{{2.6.3}{11}{Additive Smoothening}{subsubsection.2.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A new sample from class $+$ and the features $\textbf  {x} = \text  {[yellow, square]}$ that is to be classified using the training data in Figure \ref  {fig:toy_dataset}.\relax }}{11}{figure.caption.7}}
\newlabel{fig:new_sample2}{{6}{11}{A new sample from class $+$ and the features $\textbf {x} = \text {[yellow, square]}$ that is to be classified using the training data in Figure \ref {fig:toy_dataset}.\relax }{figure.caption.7}{}}
\citation{yao2013rotation}
\@writefile{toc}{\contentsline {section}{\numberline {3}Naive Bayes and Text Classification}{12}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Bag of Words Model}{12}{subsection.3.1}}
\newlabel{sec:the_bag_of_words_model}{{3.1}{12}{The Bag of Words Model}{subsection.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Bag of words representation of two sample documents $D_1$ and $D_2$.\relax }}{13}{table.caption.8}}
\newlabel{fig:bag_of_words}{{1}{13}{Bag of words representation of two sample documents $D_1$ and $D_2$.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Tokenization}{13}{subsubsection.3.1.1}}
\newlabel{sec:tokenization}{{3.1.1}{13}{Tokenization}{subsubsection.3.1.1}{}}
\citation{porter1980algorithm}
\citation{toman2006influence}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example of tokenization.\relax }}{14}{table.caption.9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Stop Words}{14}{subsubsection.3.1.2}}
\newlabel{sec:stopwords}{{3.1.2}{14}{Stop Words}{subsubsection.3.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Example of stop word removal.\relax }}{14}{table.caption.10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Stemming and Lemmatization}{14}{subsubsection.3.1.3}}
\newlabel{sec:stemming_and_lemmatization}{{3.1.3}{14}{Stemming and Lemmatization}{subsubsection.3.1.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Example of Porter stemming.\relax }}{14}{table.caption.11}}
\citation{zevcevic2011n}
\citation{kevselj2003n}
\citation{kanaris2007words}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Example of lemmatization.\relax }}{15}{table.caption.12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}\emph  {N}-grams}{15}{subsubsection.3.1.4}}
\newlabel{sec:n-grams}{{3.1.4}{15}{\emph {N}-grams}{subsubsection.3.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Decision Rule for Spam Classification}{15}{subsection.3.2}}
\newlabel{sec:decision_rule_spam}{{3.2}{15}{The Decision Rule for Spam Classification}{subsection.3.2}{}}
\citation{mccallum1998comparison}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Mutli-variate Bernoulli Naive Bayes}{16}{subsection.3.3}}
\newlabel{sec:bernoulli_bayes}{{3.3}{16}{Mutli-variate Bernoulli Naive Bayes}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Multinomial Naive Bayes}{17}{subsection.3.4}}
\newlabel{sec:multinomial_bayes}{{3.4}{17}{Multinomial Naive Bayes}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Term Frequency}{17}{subsubsection.3.4.1}}
\newlabel{sec:term_frequency}{{3.4.1}{17}{Term Frequency}{subsubsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Term Frequency - Inverse Document Frequency (Tf-idf)}{18}{subsubsection.3.4.2}}
\newlabel{sec:tf-idf}{{3.4.2}{18}{Term Frequency - Inverse Document Frequency (Tf-idf)}{subsubsection.3.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Variants of the Naive Bayes Model}{18}{section.4}}
\newlabel{sec:naive_bayes_variants}{{4}{18}{Variants of the Naive Bayes Model}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Continuous Variables}{18}{subsection.4.1}}
\newlabel{sec:continuous_variables}{{4.1}{18}{Continuous Variables}{subsection.4.1}{}}
\bibdata{../references/bayes}
\bibcite{rish2001empirical}{1}
\bibcite{domingos1997optimality}{2}
\bibcite{kazmierska2008application}{3}
\bibcite{wang2007naive}{4}
\bibcite{sahami1998bayesian}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Eager and Lazy Learning Algorithms}{19}{subsection.4.2}}
\newlabel{sec:eager_and_lazy}{{4.2}{19}{Eager and Lazy Learning Algorithms}{subsection.4.2}{}}
\bibcite{zhang2004optimality}{6}
\bibcite{yao2013rotation}{7}
\bibcite{porter1980algorithm}{8}
\bibcite{toman2006influence}{9}
\bibcite{zevcevic2011n}{10}
\bibcite{kevselj2003n}{11}
\bibcite{kanaris2007words}{12}
\bibcite{mccallum1998comparison}{13}
\bibstyle{unsrt}
