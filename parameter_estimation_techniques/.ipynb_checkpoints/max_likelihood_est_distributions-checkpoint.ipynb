{
 "metadata": {
  "name": "",
  "signature": "sha256:aab7071a5997dc4d62b20f76333d3df9e565ef9f92451425d3bead2794525324"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sebastian Raschka   \n",
      "last updated: 04/22/2014  \n",
      "\n",
      "[Link to this IPython Notebook on GitHub](https://github.com/rasbt/pattern_classification/blob/master/stat_pattern_class/supervised/parametric/parameter_estimation/max_likelihood_est_distributions.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr>\n",
      "I am really looking forward to your comments and suggestions to improve and extend this tutorial! Just send me a quick note   \n",
      "via Twitter: [@rasbt](https://twitter.com/rasbt)  \n",
      "or Email: [bluewoodtree@gmail.com](mailto:bluewoodtree@gmail.com)\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>\n",
      "<a name='introduction'></a>\n",
      "# Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Popular applications for Maximum Likelihood Estimates are the typical statistical pattern classification tasks, and in the past, I posted some [examples](https://github.com/rasbt/pattern_classification#param) using Bayes' classifiers for which the **probabilistic models and parameters were known**. In those cases, the design of the classifier was rather easy, however, in real applications, we are rarely given this information; this is where the Maximum Likelihood Estimate comes into play."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An introduction about how to use the Maximum Likelihood Estimate for pattern classification task can be found in an [earlier article](http://nbviewer.ipython.org/github/rasbt/pattern_classification/blob/master/parameter_estimation_techniques/maximum_likelihood_estimate.ipynb?create=1)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, the Maximum Likelihood Estimate still **requires partial knowledge** about the problem: We have to assume that the **model of the class conditional densities is known** (e.g., that the data follows typical Gaussian distribution). In contrast, non-parametric approaches like the Parzen-window technqiue do not require prior information about the distribution of the data (I will discuss this technique in more detail in a future article, the IPython notebook is already in preparation).  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**To summarize the problem:** Using MLE, we want to estimate the values of the parameters of a given distribution for the class-conditional densities, for example, the *mean* and *variance* assuming that the class-conditional densities are *normal*  distributed (Gaussian) with $p(\\pmb x \\; | \\; \\omega_i) \\sim N(\\mu, \\sigma^2)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#General Concept"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the Maximum Likelihood Estimate (MLE), we assume that we have a set of *i.i.d.* (independent and identically distributed) samples    \n",
      "$D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} $."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>\n",
      "### Likelihood of $ \\pmb \\theta $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The probability of observing $D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} $ is: \n",
      "<br>\n",
      "<br>\n",
      "$p(D\\; | \\;  \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;...  \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;)$  \n",
      "<br>\n",
      "Where $p(D\\; | \\;  \\pmb  \\theta\\;)$ is also called the ***likelihood of $\\pmb\\ \\theta$***."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>\n",
      "# Multivariate Gaussian Distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Probability Density Function:\n",
      "\n",
      "$ p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) $\n",
      "\n",
      "$ p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg]$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$\\Rightarrow p(D\\; | \\;  \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) \\\\\n",
      "\\Rightarrow p(D\\; | \\;  \\pmb \\theta\\;) =  \\prod_{k=1}^{n} \\; \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg]$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Natural logarithm:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$ l(\\pmb\\theta) =  \\sum\\limits_{k=1}^{n} - \\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\pmb \\Sigma^{-1} \\; (\\pmb x - \\pmb \\mu) - \\frac{d}{2} \\; ln \\; 2\\pi - \\frac{1}{2} \\;ln \\; |\\pmb\\Sigma|$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The 2 parameters that we want to estimate are $\\pmb \\mu_i$ and $\\pmb \\Sigma_i$, are   \n",
      "\n",
      "\n",
      "$\\pmb \\theta_i = \\bigg[ \\begin{array}{c}\n",
      "\\ \\theta_{i1} \\\\\n",
      "\\ \\theta_{i2} \\\\\n",
      "\\end{array} \\bigg]=\n",
      "\\bigg[ \\begin{array}{c}\n",
      "\\pmb \\mu_i \\\\\n",
      "\\pmb \\Sigma_i \\\\\n",
      "\\end{array} \\bigg]$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>\n",
      "### Maximum Likelihood Estimate (MLE):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to obtain the MLE $\\boldsymbol{\\hat{\\theta}}$, we maximize $l (\\pmb  \\theta)$, which can be done via differentiation:\n",
      "\n",
      "with \n",
      "$\\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix}  \n",
      "\\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \n",
      "\\frac{\\partial \\; }{\\partial \\; \\theta_2}\n",
      "\\end{bmatrix} = \\begin{bmatrix} \n",
      "\\frac{\\partial \\; }{\\partial \\; \\pmb \\mu} \\\\ \n",
      "\\frac{\\partial \\; }{\\partial \\; \\pmb \\sigma}\n",
      "\\end{bmatrix}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\Rightarrow \\nabla_{\\pmb \\theta} l = \\sum\\limits_{k=1}^n \\nabla_{\\pmb \\theta} \\;ln\\; p(\\pmb x| \\pmb \\theta) = 0 $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1st parameter $\\theta_1 = \\pmb \\mu$\n",
      "\n",
      "${\\hat{\\pmb\\mu}} = \\frac{1}{n} \\sum\\limits_{k=1}^{n} \\pmb x_k$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2nd parameter $\\theta_2 = \\Sigma$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "${\\hat{\\pmb\\Sigma}} = \\frac{1}{n} \\sum\\limits_{k=1}^{n} (\\pmb x_k - \\hat{\\mu})(\\pmb x_k - \\hat{\\mu})^t$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sample training data for MLE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\pmb \\mu = \\Bigg[ \\begin{array}{c}\n",
      "\\ 0 \\\\\n",
      "\\ 0 \\\\\n",
      "\\ 0\n",
      "\\end{array} \\Bigg]\\;, \\quad \\quad \n",
      "\\pmb \\Sigma = \\Bigg[ \\begin{array}{ccc}\n",
      "\\ 1 & 0 & 0 \\\\\n",
      "\\ 0 & 1 & 0 \\\\\n",
      "\\ 0 & 0 & 1\n",
      "\\end{array} \\Bigg] \\quad$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Code for multivariate Gaussian MLE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loading packages\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "%pylab inline\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# true parameters and 100 3D training data points\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "mu_vec = np.array([[0],[0],[0]])\n",
      "cov_mat = np.eye(3)\n",
      "\n",
      "multi_gauss = np.random.multivariate_normal(mu_vec.ravel(), cov_mat, 100)\n",
      "print('Dimensions: {}x{}'.format(multi_gauss.shape[0], multi_gauss.shape[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dimensions: 100x3\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mle_gaussmean(samples):\n",
      "    \"\"\"\n",
      "    Calculates the Maximum Likelihood Estimate for a mean vector\n",
      "    from a multivariate Gaussian distribution.\n",
      "    \n",
      "    Keyword arguments:\n",
      "        samples (numpy array): Training samples for the MLE.\n",
      "            Every sample point represents a row; dimensions by column.\n",
      "            \n",
      "    Returns a row vector (numpy.array) as the MLE mean estimate.\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    dimensions = samples.shape[1]\n",
      "    mu_est = np.zeros((dimensions,1))\n",
      "    for dim in range(dimensions):\n",
      "        mu_est = np.zeros((dimensions,1))\n",
      "        col_mean = sum(samples[:,dim])/len(samples[:,dim])\n",
      "        mu_est[dim] = col_mean\n",
      "    return mu_est"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import prettytable\n",
      "\n",
      "mu_est1 = np.array([[sum(x1_samples[:,0])/len(x1_samples[:,0])],[sum(x1_samples[:,1])/len(x1_samples[:,1])]])\n",
      "mu_est2 = np.array([[sum(x2_samples[:,0])/len(x2_samples[:,0])],[sum(x2_samples[:,1])/len(x2_samples[:,1])]])\n",
      "mu_est3 = np.array([[sum(x3_samples[:,0])/len(x3_samples[:,0])],[sum(x3_samples[:,1])/len(x3_samples[:,1])]])\n",
      "\n",
      "mu_mle = prettytable.PrettyTable([\"\", \"mu_1\", \"mu_2\", \"mu_3\"])\n",
      "mu_mle.add_row([\"MLE\",mu_est1, mu_est2, mu_est3])\n",
      "mu_mle.add_row([\"actual\",mu_vec1, mu_vec2, mu_vec3])\n",
      "\n",
      "print(mu_mle)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def pdf_multivariate_gauss(x, mu, cov):\n",
      "    '''\n",
      "    Caculate the multivariate normal density (pdf)\n",
      "\n",
      "    Keyword arguments:\n",
      "        x = numpy array of a \"d x 1\" sample vector\n",
      "        mu = numpy array of a \"d x 1\" mean vector\n",
      "        cov = \"numpy array of a d x d\" covariance matrix\n",
      "    '''\n",
      "    assert(mu.shape[0] > mu.shape[1]), 'mu must be a row vector'\n",
      "    assert(x.shape[0] > x.shape[1]), 'x must be a row vector'\n",
      "    assert(cov.shape[0] == cov.shape[1]), 'covariance matrix must be square'\n",
      "    assert(mu.shape[0] == cov.shape[0]), 'cov_mat and mu_vec must have the same dimensions'\n",
      "    assert(mu.shape[0] == x.shape[0]), 'mu and x must have the same dimensions'\n",
      "    part1 = 1 / ( ((2* np.pi)**(len(mu)/2)) * (np.linalg.det(cov)**(1/2)) )\n",
      "    part2 = (-1/2) * ((x-mu).T.dot(np.linalg.inv(cov))).dot((x-mu))\n",
      "    return float(part1 * np.exp(part2))\n",
      "\n",
      "def test_gauss_pdf():\n",
      "    x = np.array([[0],[0]])\n",
      "    mu  = np.array([[0],[0]])\n",
      "    cov = np.eye(2) \n",
      "\n",
      "    print(pdf_multivariate_gauss(x, mu, cov))\n",
      "\n",
      "    # prints 0.15915494309189535\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    test_gauss_pdf()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Univariate Rayleigh Distribution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}